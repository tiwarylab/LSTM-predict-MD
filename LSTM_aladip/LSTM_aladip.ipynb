{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "# from lossT import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatially discretized data into 20 bins\n",
    "bins=np.arange(-0.9, 1.1, 0.1)\n",
    "num_bins=len(bins)\n",
    "# Labels of all possible states in the ranges we considered.\n",
    "# For 2d systems, this is not the same as the number of representative values.\n",
    "all_combs = [i for i in range(num_bins)]\n",
    "vocab=sorted(all_combs)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Sequence length and shift in step between past (input) & future (output)\n",
    "seq_length = 100\n",
    "shift=1\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset.\n",
    "BUFFER_SIZE = 50000\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 128\n",
    "rnn_units = 1024\n",
    "\n",
    "# Training epochs\n",
    "EPOCHS=40\n",
    "\n",
    "# Prediction\n",
    "num_generate = 2000000\n",
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "# Experiment to find the best setting.\n",
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    \"\"\"\n",
    "    split sequences into input and target.\n",
    "    \"\"\"\n",
    "    input_text = chunk[:-shift]\n",
    "    target_text = chunk[shift:]\n",
    "    return input_text, target_text\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    \n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True,\n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def loss(labels, logits):\n",
    "\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "#     return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "def generate_text(pmodel, num_generate, temperature, start_string):\n",
    "    \"\"\"\n",
    "    # Define function for generating prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting the start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store the results\n",
    "    text_generated = np.empty(1)\n",
    "\n",
    "    # Here batch size = 1\n",
    "    pmodel.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        predictions = pmodel(input_eval)\n",
    "        \n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated = np.vstack((text_generated, idx2char[predicted_id].tolist()))\n",
    "        \n",
    "    return text_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = 'DATA_aladip/COLVAR_T450'\n",
    "phi, psi=np.loadtxt(infile, unpack=True, usecols=(1,2), skiprows=7)\n",
    "\n",
    "cos_phi=np.cos(phi)\n",
    "sin_phi=np.sin(phi)\n",
    "cos_psi=np.cos(psi)\n",
    "sin_psi=np.sin(psi)\n",
    "\n",
    "# Spatially discretized data\n",
    "idx_sin_phi=np.digitize(sin_phi, bins)\n",
    "idx_sin_psi=np.digitize(sin_psi, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2d=list(idx_sin_phi[:10000])\n",
    "text = idx_2d\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab)} # Mapping from characters to indices\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+shift, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the same trajectory as the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sin_phi_v=np.digitize(sin_phi, bins)\n",
    "idx_2dv=list(idx_sin_phi_v)\n",
    "\n",
    "vali = idx_2dv[:200000]\n",
    "vali_as_int = np.array([char2idx[c] for c in vali])\n",
    "\n",
    "# Create validation examples/targets\n",
    "vali_dataset = tf.data.Dataset.from_tensor_slices(vali_as_int)\n",
    "\n",
    "sequences = vali_dataset.batch(seq_length+shift, drop_remainder=True)\n",
    "vdataset = sequences.map(split_input_target)\n",
    "vdataset = vdataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the same trajectory and use the first few to activate the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sin_phi_p=np.digitize(sin_phi, bins)\n",
    "idx_2dp=list(idx_sin_phi_p)\n",
    "text4activation = idx_2dp[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide whether to use GPU and build model of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (64, None, 128)           2560      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (64, None, 1024)          4722688   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (64, None, 20)            20500     \n",
      "=================================================================\n",
      "Total params: 4,745,748\n",
      "Trainable params: 4,745,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNLSTM\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(\n",
    "    tf.keras.layers.LSTM, recurrent_activation='sigmoid')\n",
    "    \n",
    "    model = build_model(vocab_size = vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        rnn_units=rnn_units,\n",
    "        batch_size=BATCH_SIZE)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer = tf.train.AdamOptimizer(), loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_epoch = len(text)//(seq_length+shift)\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "v_examples=len(vali_as_int)//(seq_length+shift)\n",
    "v_steps_per_epoch=v_examples//BATCH_SIZE\n",
    "\n",
    "history = model.fit(dataset.repeat(EPOCHS), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, validation_data=vdataset.repeat(EPOCHS), validation_steps=v_steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (1, None, 128)            2560      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (1, None, 1024)           4722688   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (1, None, 20)             20500     \n",
      "=================================================================\n",
      "Total params: 4,745,748\n",
      "Trainable params: 4,745,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "length of seed: 1000\n"
     ]
    }
   ],
   "source": [
    "# Rebuild model with batch_size=1:\n",
    "pmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "pmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "pmodel.build(tf.TensorShape([1, None]))\n",
    "print(pmodel.summary())\n",
    "\n",
    "# Print the length of seed for activating the model\n",
    "print('length of seed: {}'.format(len(text4activation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start0 = time.time()\n",
    "prediction=generate_text(pmodel, num_generate, temperature, start_string=text4activation)\n",
    "print ('Time taken for total {} sec\\n'.format(time.time() - start0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('prediction',prediction[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
